{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Extraction and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# packages for preprocessing news headlines\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.util import bigrams \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections\n",
    "from collections import Counter\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for analysis - do not run for preprocessing - just keeping handy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='pyLDAvis')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Excel files for each year of Reuters Newswire headlines\n",
    "path_to_data = \"/Users/varsha/GWU_SPRING2019/Topics in Big Data/Project/Code/\"\n",
    "#newsyear = ['Reuters Newswire 2015.xlsx','Reuters Newswire 2016.xlsx','Reuters Newswire 2017.xlsx']\n",
    "newsyear = ['Reuters Newswire 2017.xlsx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processfile(datapath, filename):\n",
    "    # Read in Excel file\n",
    "    yearfile = pd.ExcelFile(join(datapath, filename))\n",
    "    \n",
    "    #Parse sheet 1, drop times from datetime, group by day\n",
    "    sheet1 = yearfile.parse('Sheet1')\n",
    "    sheet1['publish_time'] = sheet1['publish_time'].floordiv(10000)\n",
    "    daygroups1 = sheet1.groupby(['publish_time'])['headline_text'].apply(lambda x:' | '.join(x.astype(str))).reset_index()\n",
    "    \n",
    "    # If sheet 2 exists\n",
    "    try:\n",
    "        #Parse sheet 2, drop time from datetime, group by day\n",
    "        sheet2 = yearfile.parse('Sheet2')\n",
    "        sheet2['publish_time'] = sheet2['publish_time'].floordiv(10000)\n",
    "        daygroups2 = sheet2.groupby(['publish_time'])['headline_text'].apply(lambda x:' | '.join(x.astype(str))).reset_index()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #Append data from sheet 2 to sheet 1, if sheet 2 exists\n",
    "    try:\n",
    "        headlines_year = daygroups1.append(daygroups2, ignore_index=True)\n",
    "    except:\n",
    "        headlines_year = daygroups1\n",
    "    \n",
    "    #Store dataframe for year in list\n",
    "    appendedheadlines.append(headlines_year)\n",
    "    return appendedheadlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each year and process its Excel file, then concatenate into one dataframe\n",
    "appendedheadlines = []\n",
    "for year in newsyear:\n",
    "    processfile(path_to_data, year)\n",
    "\n",
    "df_headlines = pd.concat(appendedheadlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index of complete dataframe\n",
    "headlines = df_headlines.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format publish_time as a datetime\n",
    "headlines['publish_time'] = pd.to_datetime(headlines['publish_time'], format='%Y%m%d')\n",
    "# Note that time still included, just not shown due to format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date column - no time included\n",
    "headlines['date'] = headlines['publish_time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original date-time column\n",
    "headlines = headlines.drop(['publish_time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China's brokerages told to manage reputation r...</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kia Motors says plans to sell 3.17 million veh...</td>\n",
       "      <td>2017-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Around 60 killed in Brazil prison riot - state...</td>\n",
       "      <td>2017-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BRIEF-Hunter Hall International updates on off...</td>\n",
       "      <td>2017-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russia's Rosneftegaz closes Rosneft privatisat...</td>\n",
       "      <td>2017-01-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text        date\n",
       "0  China's brokerages told to manage reputation r...  2017-01-01\n",
       "1  Kia Motors says plans to sell 3.17 million veh...  2017-01-02\n",
       "2  Around 60 killed in Brazil prison riot - state...  2017-01-03\n",
       "3  BRIEF-Hunter Hall International updates on off...  2017-01-04\n",
       "4  Russia's Rosneftegaz closes Rosneft privatisat...  2017-01-05"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of classes for natural language processing\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding list of stopwords to be removed by including words specific to Reuters data - \n",
    "# observed words Reuters seems to use to indicate types of news but that do not carry news content\n",
    "#user_defined_stop_words = ['ADVISORY', 'ALERT', 'ANALYSIS', 'BRIEF', 'COLUMN', 'CORRECTED', 'DIARY', 'EMBARGOED', \n",
    "#                           'EXCLUSIVE', 'FEATURE', 'FRAUD ALERT', 'GRAPHIC',\n",
    "#                           'INSIGHT', 'INVESTIGATION ALERT', 'INVESTOR ALERT', 'PREVIEW', 'SHAREHOLDER ALERT', \n",
    "#                           'UPDATE', 'UPDATE 1', 'UPDATE 2', 'UPDATE 3', 'Jan', 'Feb', 'Mar', 'Apr', 'Jun', 'Jul', \n",
    "#                           'Aug', 'Sep', 'Oct', 'Nov', 'Dec', ' R ', ' TM ', ' plc ', ' LLC ', ' PLC ', ' CES '\n",
    "#                           'PRESS DIGEST', 'GLOBAL' 'ETF Net Asset Value'] \n",
    "                    \n",
    "user_defined_stop_words = ['UPDATE', 'Jan', 'Feb', 'Mar', 'Apr', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', \n",
    "                           ' R ', ' TM ', ' plc ', ' LLC ', ' PLC ', ' CES '] \n",
    "\n",
    "# Could combine:\n",
    "#a = nltk.corpus.stopwords.words('english')\n",
    "#b = list(string.punctuation) + user_defined_stop_words\n",
    "#stopwords = set(a).union(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    punkt_sentences = sentence_tokenizer.tokenize(x)\n",
    "    sentences_words = [treebank_tokenizer.tokenize(sentence) for sentence in punkt_sentences] #segment sentences\n",
    "    all_tokens = [word for sentence in sentences_words for word in sentence] #tokenize words\n",
    "    clean_tokens = [w for w in all_tokens if w not in user_defined_stop_words] #drop Reuters stopwords\n",
    "    #tokens = [word.lower() for word in clean_tokens]# make lowercase\n",
    "    content = [w for w in clean_tokens if w.lower() not in stopwords] #drop regular stopwords\n",
    "    content2 = [words for words in content if words.isalpha()] #drop punctuation\n",
    "    lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in content2] #lemmatize\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess headlines for each date\n",
    "headlines['clean_text'] = headlines['headline_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of each date\n",
    "dates = list(headlines['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list from each row of cleaned text from dataframe\n",
    "cleantextlist = list(headlines['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip lists together and save each date's news headlines as a text file.\n",
    "for i, t in zip(dates, cleantextlist):\n",
    "    file = open('/Users/varsha/GWU_SPRING2019/Topics in Big Data/Project/Code/Daily News/' + str(i) + '.txt', 'w',encoding='utf-8')\n",
    "    file.write(t)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/varsha/GWU_SPRING2019/Topics in Big Data/Project/Code/Daily News'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspath = os.path.join(os.getcwd(), 'Code','Daily News')\n",
    "newspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/varsha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/varsha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "article_hash = {}\n",
    "directory = \"/Users/varsha/GWU_SPRING2019/Topics in Big Data/Project/Code/Daily News/\"\n",
    "\n",
    "files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "\n",
    "for file in files:\n",
    "    #file = pathlib.Path( \"/Users/varsha/Python Workspace/homework_2/data\")\n",
    "    #print(file)\n",
    "    fileop=open(directory+\"/\"+file,\"r\")\n",
    "    text = fileop.read()\n",
    "    article_hash[file]  = text\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"file;\" , file)\n",
    "    print(article_hash[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(text):\n",
    "    def lemmatize_stemming(text):\n",
    "        stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "    def preprocess(text):\n",
    "        result = []\n",
    "        for token in gensim.utils.simple_preprocess(text):\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                #result.append(token)\n",
    "                result.append(lemmatize_stemming(token))\n",
    "        return result\n",
    "\n",
    "\n",
    "    words = []\n",
    "    for word in text.split(' '):\n",
    "        words.append(word)\n",
    "\n",
    "    processed_data = preprocess(text)\n",
    "\n",
    "    dictionary = gensim.corpora.Dictionary([processed_data])\n",
    "\n",
    "    bow_corpus = [dictionary.doc2bow(processed_data)]\n",
    "\n",
    "    bow_doc_0 = bow_corpus[0]\n",
    "\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "    # LDA Model using Bag of Words\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=15, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "    topic_score_list = lda_model.show_topics(num_topics=1, num_words=15, log=False, formatted=False)[0][1]\n",
    "    topics_list = [topic[0] for topic in topic_score_list]\n",
    "    return(topics_list)\n",
    "    #return(lda_model.show_topics(num_topics=1, num_words=15, log=False, formatted=False)[0][1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_afinn_score(text):\n",
    "    afinn = Afinn(emoticons=True)\n",
    "\n",
    "    #compute sentiment scores (polarity) and labels\n",
    "    sentiment_score     = [afinn.score(text)]\n",
    "    sentiment_category  = ['positive' if score > 0\n",
    "                           else 'negative' if score < 0\n",
    "                           else 'neutral'\n",
    "                           for score in sentiment_score]\n",
    "\n",
    "    #print(sentiment_score)\n",
    "    #print(sentiment_category)\n",
    "    return(sentiment_score, sentiment_category)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(source,text):\n",
    "    article_df = pd.DataFrame(columns=['source','topics'])      # initialize dataframe for each article\n",
    "    #person_names=person_list\n",
    "\n",
    "    #article_df['a'] = None\n",
    "\n",
    "   \n",
    "    #article_df['source'] = pd.Series(dtype='str')\n",
    "    #article_df['source'] = source\n",
    "    \n",
    "    topics = get_topics(text)\n",
    "    article_df['topics'] = [topics]\n",
    "    article_df['source'] = source\n",
    "    (sentiment_score, sentiment_category) = compute_afinn_score(text)\n",
    "    article_df['sentiment_score'] = sentiment_score\n",
    "    article_df['sentiment_category'] = sentiment_category\n",
    "    return article_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "from afinn import Afinn\n",
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsfiles = [f for f in listdir(newspath) if isfile(join(newspath, f))]\n",
    "\n",
    "for file in newsfiles:\n",
    "   filepath = newspath + \"/\" + file\n",
    "   data = open(filepath,'r',encoding='utf-8')\n",
    "   text = data.read()\n",
    "   results_df = results_df.append(process_text(file,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>topics</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-24.txt</td>\n",
       "      <td>[share, profit, group, offer, plan, bank, mill...</td>\n",
       "      <td>645.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-30.txt</td>\n",
       "      <td>[profit, share, loss, rise, india, bank, fall,...</td>\n",
       "      <td>607.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-18.txt</td>\n",
       "      <td>[share, profit, trump, rise, group, announc, m...</td>\n",
       "      <td>760.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-11.txt</td>\n",
       "      <td>[trump, report, stake, profit, share, hold, ri...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-05.txt</td>\n",
       "      <td>[trump, preview, stand, travel, result, judg, ...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-28.txt</td>\n",
       "      <td>[share, unit, china, say, group, yuan, hold, b...</td>\n",
       "      <td>521.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-14.txt</td>\n",
       "      <td>[bank, say, share, profit, group, unit, deal, ...</td>\n",
       "      <td>936.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-05.txt</td>\n",
       "      <td>[share, announc, stock, bank, price, sale, mar...</td>\n",
       "      <td>197.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-11.txt</td>\n",
       "      <td>[share, bank, sale, deal, group, announc, repo...</td>\n",
       "      <td>244.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-21.txt</td>\n",
       "      <td>[share, profit, million, announc, bank, report...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-18.txt</td>\n",
       "      <td>[announc, share, group, india, agreement, stoc...</td>\n",
       "      <td>668.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-30.txt</td>\n",
       "      <td>[result, million, china, trump, unit, share, h...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-24.txt</td>\n",
       "      <td>[result, trump, championship, vote, preview, s...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-09.txt</td>\n",
       "      <td>[trump, motor, time, test, deal, china, tour, ...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-08.txt</td>\n",
       "      <td>[june, trump, china, agreement, deal, enter, m...</td>\n",
       "      <td>283.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-25.txt</td>\n",
       "      <td>[announc, share, unit, group, firm, plan, bank...</td>\n",
       "      <td>291.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-19.txt</td>\n",
       "      <td>[share, announc, unit, million, group, appoint...</td>\n",
       "      <td>216.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-20.txt</td>\n",
       "      <td>[share, profit, bank, earn, report, announc, j...</td>\n",
       "      <td>1364.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-10.txt</td>\n",
       "      <td>[share, sale, sign, bank, group, announc, chin...</td>\n",
       "      <td>444.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-04.txt</td>\n",
       "      <td>[announc, bank, sale, share, price, group, fun...</td>\n",
       "      <td>269.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-15.txt</td>\n",
       "      <td>[share, bank, say, announc, unit, group, agree...</td>\n",
       "      <td>799.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-01.txt</td>\n",
       "      <td>[share, say, report, announc, sale, bank, chin...</td>\n",
       "      <td>338.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-29.txt</td>\n",
       "      <td>[unit, china, say, share, stake, bank, year, y...</td>\n",
       "      <td>670.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-04.txt</td>\n",
       "      <td>[trump, profit, share, rise, report, bank, sta...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-19.txt</td>\n",
       "      <td>[share, announc, china, bank, trump, group, mi...</td>\n",
       "      <td>521.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-10.txt</td>\n",
       "      <td>[profit, share, rise, trump, report, announc, ...</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-31.txt</td>\n",
       "      <td>[share, profit, bank, announc, loss, million, ...</td>\n",
       "      <td>391.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-25.txt</td>\n",
       "      <td>[share, profit, announc, trump, bank, china, u...</td>\n",
       "      <td>794.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-27.txt</td>\n",
       "      <td>[result, final, trump, stand, china, champions...</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-06.txt</td>\n",
       "      <td>[share, profit, announc, stake, report, hold, ...</td>\n",
       "      <td>734.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-16.txt</td>\n",
       "      <td>[share, profit, report, loss, million, bank, z...</td>\n",
       "      <td>787.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-23.txt</td>\n",
       "      <td>[share, report, profit, earn, million, loss, b...</td>\n",
       "      <td>1424.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-28.txt</td>\n",
       "      <td>[trump, result, french, lead, monaco, stand, c...</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-21.txt</td>\n",
       "      <td>[share, profit, report, group, announc, bank, ...</td>\n",
       "      <td>857.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-09.txt</td>\n",
       "      <td>[profit, share, report, group, rise, million, ...</td>\n",
       "      <td>1482.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-14.txt</td>\n",
       "      <td>[china, profit, rise, cyber, korea, silk, road...</td>\n",
       "      <td>246.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-24.txt</td>\n",
       "      <td>[preview, china, lead, test, past, soccer, min...</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-30.txt</td>\n",
       "      <td>[year, china, say, unit, share, bank, stake, a...</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-18.txt</td>\n",
       "      <td>[announc, share, say, unit, china, invest, dea...</td>\n",
       "      <td>653.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-21.txt</td>\n",
       "      <td>[share, profit, result, bank, unit, stock, fit...</td>\n",
       "      <td>376.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-09.txt</td>\n",
       "      <td>[bank, group, sale, share, china, stock, india...</td>\n",
       "      <td>135.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-14.txt</td>\n",
       "      <td>[profit, share, announc, china, bank, hold, gr...</td>\n",
       "      <td>495.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-28.txt</td>\n",
       "      <td>[announc, share, unit, bank, million, group, p...</td>\n",
       "      <td>764.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-05.txt</td>\n",
       "      <td>[june, bank, share, china, group, announc, sig...</td>\n",
       "      <td>407.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-11.txt</td>\n",
       "      <td>[share, announc, rise, bank, profit, juli, inv...</td>\n",
       "      <td>655.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-10.txt</td>\n",
       "      <td>[share, june, bank, juli, trade, hold, unit, a...</td>\n",
       "      <td>694.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-04.txt</td>\n",
       "      <td>[bank, share, june, group, china, unit, juli, ...</td>\n",
       "      <td>496.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-29.txt</td>\n",
       "      <td>[bank, loss, share, announc, hold, profit, mil...</td>\n",
       "      <td>358.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-01.txt</td>\n",
       "      <td>[share, loss, profit, harvey, million, bank, s...</td>\n",
       "      <td>756.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-15.txt</td>\n",
       "      <td>[announc, bank, share, unit, stock, china, pri...</td>\n",
       "      <td>291.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-08.txt</td>\n",
       "      <td>[nate, preview, open, hurrican, gulf, trump, f...</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-20.txt</td>\n",
       "      <td>[share, profit, report, bank, group, earn, uni...</td>\n",
       "      <td>956.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-19.txt</td>\n",
       "      <td>[announc, say, share, unit, bank, group, plan,...</td>\n",
       "      <td>614.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-31.txt</td>\n",
       "      <td>[state, china, preview, dead, crash, year, pro...</td>\n",
       "      <td>-266.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-25.txt</td>\n",
       "      <td>[unit, yuan, china, stake, share, technolog, f...</td>\n",
       "      <td>308.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-15.txt</td>\n",
       "      <td>[profit, loss, share, post, report, million, g...</td>\n",
       "      <td>804.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-08.txt</td>\n",
       "      <td>[share, profit, report, trump, bank, dividend,...</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-01.txt</td>\n",
       "      <td>[report, share, april, earn, group, agreement,...</td>\n",
       "      <td>311.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-05-29.txt</td>\n",
       "      <td>[profit, share, rise, bank, loss, hold, millio...</td>\n",
       "      <td>360.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-20.txt</td>\n",
       "      <td>[share, profit, group, unit, sign, dividend, t...</td>\n",
       "      <td>481.0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            source                                             topics  \\\n",
       "0   2017-05-24.txt  [share, profit, group, offer, plan, bank, mill...   \n",
       "0   2017-05-30.txt  [profit, share, loss, rise, india, bank, fall,...   \n",
       "0   2017-05-18.txt  [share, profit, trump, rise, group, announc, m...   \n",
       "0   2017-02-11.txt  [trump, report, stake, profit, share, hold, ri...   \n",
       "0   2017-02-05.txt  [trump, preview, stand, travel, result, judg, ...   \n",
       "0   2017-12-28.txt  [share, unit, china, say, group, yuan, hold, b...   \n",
       "0   2017-12-14.txt  [bank, say, share, profit, group, unit, deal, ...   \n",
       "0   2017-10-05.txt  [share, announc, stock, bank, price, sale, mar...   \n",
       "0   2017-10-11.txt  [share, bank, sale, deal, group, announc, repo...   \n",
       "0   2017-07-21.txt  [share, profit, million, announc, bank, report...   \n",
       "0   2017-09-18.txt  [announc, share, group, india, agreement, stoc...   \n",
       "0   2017-09-30.txt  [result, million, china, trump, unit, share, h...   \n",
       "0   2017-09-24.txt  [result, trump, championship, vote, preview, s...   \n",
       "0   2017-07-09.txt  [trump, motor, time, test, deal, china, tour, ...   \n",
       "0   2017-07-08.txt  [june, trump, china, agreement, deal, enter, m...   \n",
       "0   2017-09-25.txt  [announc, share, unit, group, firm, plan, bank...   \n",
       "0   2017-09-19.txt  [share, announc, unit, million, group, appoint...   \n",
       "0   2017-07-20.txt  [share, profit, bank, earn, report, announc, j...   \n",
       "0   2017-10-10.txt  [share, sale, sign, bank, group, announc, chin...   \n",
       "0   2017-10-04.txt  [announc, bank, sale, share, price, group, fun...   \n",
       "0   2017-12-15.txt  [share, bank, say, announc, unit, group, agree...   \n",
       "0   2017-12-01.txt  [share, say, report, announc, sale, bank, chin...   \n",
       "0   2017-12-29.txt  [unit, china, say, share, stake, bank, year, y...   \n",
       "0   2017-02-04.txt  [trump, profit, share, rise, report, bank, sta...   \n",
       "0   2017-05-19.txt  [share, announc, china, bank, trump, group, mi...   \n",
       "0   2017-02-10.txt  [profit, share, rise, trump, report, announc, ...   \n",
       "0   2017-05-31.txt  [share, profit, bank, announc, loss, million, ...   \n",
       "0   2017-05-25.txt  [share, profit, announc, trump, bank, china, u...   \n",
       "0   2017-05-27.txt  [result, final, trump, stand, china, champions...   \n",
       "0   2017-02-06.txt  [share, profit, announc, stake, report, hold, ...   \n",
       "..             ...                                                ...   \n",
       "0   2017-05-16.txt  [share, profit, report, loss, million, bank, z...   \n",
       "0   2017-02-23.txt  [share, report, profit, earn, million, loss, b...   \n",
       "0   2017-05-28.txt  [trump, result, french, lead, monaco, stand, c...   \n",
       "0   2017-02-21.txt  [share, profit, report, group, announc, bank, ...   \n",
       "0   2017-02-09.txt  [profit, share, report, group, rise, million, ...   \n",
       "0   2017-05-14.txt  [china, profit, rise, cyber, korea, silk, road...   \n",
       "0   2017-12-24.txt  [preview, china, lead, test, past, soccer, min...   \n",
       "0   2017-12-30.txt  [year, china, say, unit, share, bank, stake, a...   \n",
       "0   2017-12-18.txt  [announc, share, say, unit, china, invest, dea...   \n",
       "0   2017-10-21.txt  [share, profit, result, bank, unit, stock, fit...   \n",
       "0   2017-10-09.txt  [bank, group, sale, share, china, stock, india...   \n",
       "0   2017-09-14.txt  [profit, share, announc, china, bank, hold, gr...   \n",
       "0   2017-09-28.txt  [announc, share, unit, bank, million, group, p...   \n",
       "0   2017-07-05.txt  [june, bank, share, china, group, announc, sig...   \n",
       "0   2017-07-11.txt  [share, announc, rise, bank, profit, juli, inv...   \n",
       "0   2017-07-10.txt  [share, june, bank, juli, trade, hold, unit, a...   \n",
       "0   2017-07-04.txt  [bank, share, june, group, china, unit, juli, ...   \n",
       "0   2017-09-29.txt  [bank, loss, share, announc, hold, profit, mil...   \n",
       "0   2017-09-01.txt  [share, loss, profit, harvey, million, bank, s...   \n",
       "0   2017-09-15.txt  [announc, bank, share, unit, stock, china, pri...   \n",
       "0   2017-10-08.txt  [nate, preview, open, hurrican, gulf, trump, f...   \n",
       "0   2017-10-20.txt  [share, profit, report, bank, group, earn, uni...   \n",
       "0   2017-12-19.txt  [announc, say, share, unit, bank, group, plan,...   \n",
       "0   2017-12-31.txt  [state, china, preview, dead, crash, year, pro...   \n",
       "0   2017-12-25.txt  [unit, yuan, china, stake, share, technolog, f...   \n",
       "0   2017-05-15.txt  [profit, loss, share, post, report, million, g...   \n",
       "0   2017-02-08.txt  [share, profit, report, trump, bank, dividend,...   \n",
       "0   2017-05-01.txt  [report, share, april, earn, group, agreement,...   \n",
       "0   2017-05-29.txt  [profit, share, rise, bank, loss, hold, millio...   \n",
       "0   2017-02-20.txt  [share, profit, group, unit, sign, dividend, t...   \n",
       "\n",
       "    sentiment_score sentiment_category  \n",
       "0             645.0           positive  \n",
       "0             607.0           positive  \n",
       "0             760.0           positive  \n",
       "0              45.0           positive  \n",
       "0              13.0           positive  \n",
       "0             521.0           positive  \n",
       "0             936.0           positive  \n",
       "0             197.0           positive  \n",
       "0             244.0           positive  \n",
       "0            1000.0           positive  \n",
       "0             668.0           positive  \n",
       "0              32.0           positive  \n",
       "0              24.0           positive  \n",
       "0              11.0           positive  \n",
       "0             283.0           positive  \n",
       "0             291.0           positive  \n",
       "0             216.0           positive  \n",
       "0            1364.0           positive  \n",
       "0             444.0           positive  \n",
       "0             269.0           positive  \n",
       "0             799.0           positive  \n",
       "0             338.0           positive  \n",
       "0             670.0           positive  \n",
       "0             161.0           positive  \n",
       "0             521.0           positive  \n",
       "0            1340.0           positive  \n",
       "0             391.0           positive  \n",
       "0             794.0           positive  \n",
       "0             -63.0           negative  \n",
       "0             734.0           positive  \n",
       "..              ...                ...  \n",
       "0             787.0           positive  \n",
       "0            1424.0           positive  \n",
       "0            -134.0           negative  \n",
       "0             857.0           positive  \n",
       "0            1482.0           positive  \n",
       "0             246.0           positive  \n",
       "0             -20.0           negative  \n",
       "0              -7.0           negative  \n",
       "0             653.0           positive  \n",
       "0             376.0           positive  \n",
       "0             135.0           positive  \n",
       "0             495.0           positive  \n",
       "0             764.0           positive  \n",
       "0             407.0           positive  \n",
       "0             655.0           positive  \n",
       "0             694.0           positive  \n",
       "0             496.0           positive  \n",
       "0             358.0           positive  \n",
       "0             756.0           positive  \n",
       "0             291.0           positive  \n",
       "0             -38.0           negative  \n",
       "0             956.0           positive  \n",
       "0             614.0           positive  \n",
       "0            -266.0           negative  \n",
       "0             308.0           positive  \n",
       "0             804.0           positive  \n",
       "0            1083.0           positive  \n",
       "0             311.0           positive  \n",
       "0             360.0           positive  \n",
       "0             481.0           positive  \n",
       "\n",
       "[365 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
